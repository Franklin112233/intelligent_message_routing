# Design: Intelligent message routing with guardrails

## Context

- **Current state**: No existing routing system; assignment provides synthetic `messages.csv`, `kb/` policy snippets, and `pii_patterns.yaml`. All processing is greenfield.
- **Constraints**: 2–3 hour build; treat all inputs as confidential; no raw text to third-party APIs without redaction; one-command run and requirements file; prefer local/open models or mocks.
- **Stakeholders**: Evaluators (assignment); future ops (routing quality, latency, cost).

## Goals / Non-Goals

**Goals:**

- Implement a thin vertical slice: **ingress → PII redaction → intent classification → draft response (≥2 intents) → evaluation/guardrails**.
- Support three classification backends: traditional ML (e.g. MTL), LLM few-shot, and hybrid (LLM orchestrator calls MTL).
- Ground draft responses in `kb/` snippets with citations; provide no-LLM fallback and confidence-based escalation.
- Deliver runnable code, README, basic metrics, redaction tests, and at least one automated draft check (masking, citation, or safety).

**Non-Goals:**

- High accuracy on all intents (baseline + improvement path is enough).
- Web UI (CLI or notebook is fine).
- Advanced retrieval (e.g. vector store); simple in-memory lookup of policy by intent is acceptable.
- Full production deployment or migration from an existing system.

## Decisions

### 1. Pipeline order

- **Decision**: Ingress → **PII redaction** → intent classification → draft response → logging/eval. Redaction runs before any call to a non-local model or external service.
- **Rationale**: Eliminates risk of sending raw PII off-host; satisfies assignment and proposal. Deterministic regex/pattern redaction is fast and auditable.
- **Alternatives**: Redact only for logging (rejected: does not protect external API calls); LLM-based redaction only (rejected for first step: need deterministic, testable baseline).

### 2. PII redaction implementation

- **Decision**: Load patterns from `pii_patterns.yaml` (or equivalent); apply regex/pattern replacement in a single pass; replace matches with a fixed token (e.g. `[REDACTED]` or type-specific placeholders). Unit tests assert that known PII in fixture text is redacted.
- **Rationale**: Assignment supplies `pii_patterns.yaml`; patterns are configurable and testable without ML.
- **Alternatives**: LLM for redaction (higher cost/latency, harder to prove completeness); external redaction service (adds dependency and network; out of scope for thin slice).

### 3. Intent classification: MTL vs LLM vs hybrid

- **Decision**: Implement **real** backends: **stub** (label lookup), **MTL** (multi-task learning in `app/mtl.py`). MTL uses shared TF-IDF + two LogisticRegression heads (intent, suggested_queue); trained on messages.csv; model persisted to `models/mtl_model.joblib`. Pipeline and eval use MTL when the model file exists, else stub. No placeholder-only paths; LLM backend can be added later with the same interface.
- **Rationale**: Proposal and comparison table define trade-offs; design requires at least one real ML path (MTL). Contract: input redacted text → output intent, suggested_queue, confidence.
- **Alternatives**: Stub-only (rejected: user requested real script/code); LLM-only (rejected: proposal requires documenting ML option).

### 4. Draft response and knowledge grounding

- **Decision**: For ≥2 intents (e.g. card lost/stolen, suspected fraud), load relevant `kb/*.md` by intent; generate draft via **template-based** reply or **LLM** (GPT-4o-mini in `app/llm.py`). When `USE_LLM=1` and `OPENAI_API_KEY` is set (e.g. in `.env`), draft is generated by the LLM with policy in context and citation requirement; on missing key or API error, fall back to template. No placeholder text; both paths produce real output.
- **Rationale**: Assignment requires policy-grounded drafts with citations and fallback; `kb/` in-memory; LLM optional, .env for key, redacted text only sent to API.
- **Alternatives**: Vector search over `kb/` (rejected); placeholder LLM branch (rejected: user requested real code).

### 5. Guardrails

- **Decision**: (1) Redaction before external call (mandatory). (2) Prompt constraints: system prompt limits role and instructs to cite policy only. (3) Post-draft checks: at least one automated check (PII masking present, policy citation present, or basic safety rule). (4) Escalation path when confidence low or check fails.
- **Rationale**: Proposal and assignment call out guardrails; minimal set for the slice.
- **Alternatives**: Grammar/formal checks (optional enhancement); full unsafe-content filter (can be stubbed with a simple keyword or regex check).

### 6. Tech stack and run model

- **Decision**: Python; `pyproject.toml` (UV); single entrypoint (e.g. `python -m app.run`). Data: `assignment/data/messages.csv`, `assignment/data/kb/`, `assignment/data/pii_patterns.yaml`. No vector DB; in-memory dict for `kb/`. MTL in `app/mtl.py` (scikit-learn). LLM draft: OpenAI GPT-4o-mini via `app/llm.py`; key from env or `.env` (python-dotenv); `USE_LLM=1` to enable.
- **Rationale**: Assignment allows choice of stack; Python fits data science take-home; one-command run and requirements are required.
- **Alternatives**: Node/TypeScript (acceptable but not assumed); managed vector DB (rejected for scope).

### 7. Policy store: in-memory vs vector DB

- **Decision**: Use **in-memory** storage for `kb/` policy snippets (e.g. load all `kb/*.md` at startup into a dict keyed by intent or filename). No vector database.
- **Rationale**: (1) We have a clear **intent → snippet** mapping: classify first, then fetch the snippet for that intent; no need for semantic “find snippets similar to this message.” (2) Scale is tiny (6–8 short files, ~300 messages). (3) Assignment says a simple in-memory store is acceptable and no advanced retrieval needed. (4) No extra dependency or embedding step; one-command run stays simple.
- **When to reconsider a vector DB**: Many/long policy documents where “which parts are relevant?” requires similarity search; open-ended intents with no fixed intent→snippet map; or frequent policy updates where semantic retrieval adds value. For this slice, in-memory is sufficient.

## Risks / Trade-offs

| Risk | Mitigation |
|------|------------|
| **Hallucination / wrong citations** | Ground on `kb/` only; automated check that citation placeholders exist; low-confidence escalation. |
| **Prompt / data exfiltration** | Never send unredacted text to external API; redact first; use local/mock when possible. |
| **MTL overfit on small data** | Use simple baseline (e.g. log-reg or small classifier); document that more data would be needed for production. |
| **Latency/cost of LLM** | Document in README; use local or mock for exercise; hybrid uses MTL for classification to limit LLM tokens. |
| **Bias / operational abuse** | Explainability (features or rationales); document residual risks in README; human escalation path. |

## Migration Plan

- **Deploy**: N/A for assignment (runnable locally or in notebook). If this were production: deploy redaction and classification first; add draft response behind feature flag; roll out by queue.
- **Rollback**: No live migration; revert code and re-run pipeline.

## Open Questions

- Exact confidence threshold for escalation (e.g. 0.7 or configurable).
- Format of citation in draft output (e.g. `[kb: filename]` vs inline footnote).

## Implementation notes (no placeholders)

- **Classification**: `app/classify.py` (interface); `app/mtl.py` (real MTL train + predict: shared TF-IDF + two LogisticRegression heads for intent and suggested_queue); stub in classify.py for fallback. Run/eval use MTL when `models/mtl_model.joblib` exists (train via `make train` or `python -m app.train_mtl`). **Train/test split**: `--train-ratio` (train_mtl) and `--test-ratio` (eval) with fixed `random_state=42` and stratified split; e.g. `make train TRAIN_RATIO=0.8` then `make eval TEST_RATIO=0.2` for holdout evaluation.
- **Draft**: `app/draft.py` produces real template-based or LLM-based text with citations. **LLM draft**: `app/llm.py` (GPT-4o-mini); set `OPENAI_API_KEY` (e.g. in `.env`) and `USE_LLM=1` to enable; only redacted text is sent; fallback to template on missing key or API error.
- **CLI (run)**: `app/run.py` is a small CLI: without `MSG`, it prompts "Enter message (or press Enter to run 5 from CSV)"; with `MSG="..."` or positional arg it runs on that single message. When `rich` is installed: progress bar for batch, tables for batch results, panels for single-message result; confidence is shown in run output.
- **Docs**: README includes quick start (steps in order with commands), project layout table, commands table (all in Makefile), holdout eval (TRAIN_RATIO / TEST_RATIO), and system architecture (mermaid). Makefile default target is `help`; all run-related commands: install, train, run, test, eval.
